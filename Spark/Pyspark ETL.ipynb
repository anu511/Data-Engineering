{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef301ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c41fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bedef8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359714f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388baede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "config = SparkConf().setMaster('local[4]').setAppName(\"ETL Pipeline\")\n",
    "sc = SparkContext(conf = config)\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88aa90fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb60029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0e782683c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ece25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF = spark.read.format(\"jdbc\")\\\n",
    ".option(\"url\",\"jdbc:mysql://localhost:3306/hremployeeDb\")\\\n",
    ".option(\"dbtable\",\"HR_Employee\").option(\"user\",\"root\").option(\"password\",\"hadoop@123\")\\\n",
    ".option(\"driver\",\"com.mysql.cj.jdbc.Driver\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3ea14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|EmployeeID|          Department|           JobRole|Attrition|Gender|Age|MaritalStatus|    Education|EducationField|   BusinessTravel|JobInvolvement|JobLevel|JobSatisfaction|Hourlyrate|Income|Salaryhike|OverTime|Workex|YearsSinceLastPromotion|EmpSatisfaction|TrainingTimesLastYear|WorkLifeBalance|Performance_Rating|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "|         1|               Sales|   Sales Executive|      Yes|Female| 41|       Single|      College| Life Sciences|    Travel_Rarely|          High|       2|      Very High|        94|  5993|        11|     Yes|     8|                      0|         Medium|                    0|            Bad|        Excellent\r",
      "|\n",
      "|         2|Research & Develo...|Research Scientist|       No|  Male| 49|      Married|Below College| Life Sciences|Travel_Frequently|        Medium|       2|         Medium|        61|  5130|        23|      No|    10|                      1|           High|                    3|         Better|      Outstanding\r",
      "|\n",
      "+----------+--------------------+------------------+---------+------+---+-------------+-------------+--------------+-----------------+--------------+--------+---------------+----------+------+----------+--------+------+-----------------------+---------------+---------------------+---------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hremployeeDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250974df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(HR_Employee) [numPartitions=1] [EmployeeID#0,Department#1,JobRole#2,Attrition#3,Gender#4,Age#5,MaritalStatus#6,Education#7,EducationField#8,BusinessTravel#9,JobInvolvement#10,JobLevel#11,JobSatisfaction#12,Hourlyrate#13,Income#14,Salaryhike#15,OverTime#16,Workex#17,YearsSinceLastPromotion#18,EmpSatisfaction#19,TrainingTimesLastYear#20,WorkLifeBalance#21,Performance_Rating#22] PushedFilters: [], ReadSchema: struct<EmployeeID:int,Department:string,JobRole:string,Attrition:string,Gender:string,Age:int,Mar...\n"
     ]
    }
   ],
   "source": [
    "# Show Physical Plan of execution which is known as DAG\n",
    "hremployeeDF.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dccc979",
   "metadata": {},
   "source": [
    "### Materialized View of Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7385fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hremployeeDF.createOrReplaceTempView('hremployee')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f91dd",
   "metadata": {},
   "source": [
    "### 1.Display shape of hremployee table \n",
    "* Show number of rows and number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd3b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cols = len(hremployeeDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04feee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|RowCount|ColCount|\n",
      "+--------+--------+\n",
      "|    1469|      23|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'''\n",
    "SELECT COUNT(*) as RowCount,{num_of_cols} as ColCount from hremployee\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2a319",
   "metadata": {},
   "source": [
    "spark.sql('''\n",
    "SELECT count(*) as ROWS,SIZE(COLLECT_LIST(*)) as COLUMNS_COUNT from hremployee\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0543b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          EmployeeID|      int|   null|\n",
      "|          Department|   string|   null|\n",
      "|             JobRole|   string|   null|\n",
      "|           Attrition|   string|   null|\n",
      "|              Gender|   string|   null|\n",
      "|                 Age|      int|   null|\n",
      "|       MaritalStatus|   string|   null|\n",
      "|           Education|   string|   null|\n",
      "|      EducationField|   string|   null|\n",
      "|      BusinessTravel|   string|   null|\n",
      "|      JobInvolvement|   string|   null|\n",
      "|            JobLevel|      int|   null|\n",
      "|     JobSatisfaction|   string|   null|\n",
      "|          Hourlyrate|      int|   null|\n",
      "|              Income|      int|   null|\n",
      "|          Salaryhike|      int|   null|\n",
      "|            OverTime|   string|   null|\n",
      "|              Workex|      int|   null|\n",
      "|YearsSinceLastPro...|      int|   null|\n",
      "|     EmpSatisfaction|   string|   null|\n",
      "|TrainingTimesLast...|      int|   null|\n",
      "|     WorkLifeBalance|   string|   null|\n",
      "|  Performance_Rating|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe hremployee\").show(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c71b3e",
   "metadata": {},
   "source": [
    "### 2.Write a query to show first three employee from each Job Role to join the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd71ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "SELECT\n",
    "    JobRole, EmployeeID\n",
    "FROM\n",
    "    (SELECT\n",
    "        JobRole,\n",
    "        EmployeeID,\n",
    "        RANK() OVER (PARTITION BY JobRole ORDER BY EmployeeID) AS rank\n",
    "        FROM\n",
    "            hremployee\n",
    "    )\n",
    "WHERE rank <= 3\n",
    "ORDER BY JobRole, rank\n",
    "''').show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d03fac",
   "metadata": {},
   "source": [
    "### 3.Write a query to show top 3 employee from each Job Role earning highest salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "SELECT\n",
    "    EmployeeID,JobRole,Income\n",
    "FROM\n",
    "    (SELECT\n",
    "        Income,\n",
    "        EmployeeID,\n",
    "        JobRole,\n",
    "        RANK() OVER (PARTITION BY JobRole ORDER BY Income desc) AS rank\n",
    "        FROM\n",
    "            hremployee\n",
    "    )\n",
    "WHERE rank <= 3\n",
    "ORDER BY JobRole, rank\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff58ef",
   "metadata": {},
   "source": [
    "### 4.Show top 3 Highest Package from overall Job Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04096fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "SELECT JobRole, Income FROM hremployee ORDER BY Income DESC LIMIT 3\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a031bb",
   "metadata": {},
   "source": [
    "### 5.Lag() - Write a Spark SQL query to show employee in order of  Ascending Order with respect to employee income compared to previous income for each job role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a63ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT EmployeeID, JobRole,Income,Previous,(Income - Previous) as Diff\n",
    "FROM\n",
    "    (SELECT\n",
    "        EmployeeID,\n",
    "        JobRole,\n",
    "        Income,\n",
    "        LAG(Income) OVER (PARTITION BY JobRole ORDER BY EmployeeID) AS Previous\n",
    "    FROM hremployee\n",
    "    )\n",
    "ORDER BY JobRole,Diff\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eecb78c",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "(SELECT\n",
    "    EmployeeID,\n",
    "    JobRole,\n",
    "    Income - LAG(Income,1,0) OVER (PARTITION BY JobRole ORDER BY EmployeeID) AS Difference\n",
    "FROM hremployee)''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83115ab",
   "metadata": {},
   "source": [
    "### Lead()\n",
    "* Row's next records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f34d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "    EmployeeID,\n",
    "    Department,\n",
    "    JobRole,\n",
    "    Age,\n",
    "    Gender,\n",
    "    Income,\n",
    "    LEAD(Income,2,0) OVER(ORDER BY EmployeeID) as lead\n",
    "FROM hremployee\n",
    "''').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85382a78",
   "metadata": {},
   "source": [
    "### NTILE()\n",
    "* Dividing Records into number of quarters(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "    EmployeeID,\n",
    "    Department,\n",
    "    JobRole,\n",
    "    Age,\n",
    "    Gender,\n",
    "    Income,\n",
    "    NTILE(4) OVER(ORDER BY Income) as salary_quartiles\n",
    "FROM hremployee\n",
    "''').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0caa8",
   "metadata": {},
   "source": [
    "### Find the Number of Employees in each percentile group 0-25th,25-50th,50-75th,75-100th using percent_rank() and create a new category using case when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) AS count\n",
    "FROM (\n",
    "    SELECT \n",
    "        EmployeeID,\n",
    "        Income,\n",
    "        percentile_rank,\n",
    "        CASE \n",
    "            WHEN percentile_rank <= 0.25 THEN '0-25th'\n",
    "            WHEN percentile_rank <= 0.50 THEN '25-50th'\n",
    "            WHEN percentile_rank <= 0.75 THEN '50-75th'\n",
    "            ELSE '75-100th'\n",
    "        END AS category\n",
    "    FROM (\n",
    "        SELECT \n",
    "            EmployeeID,\n",
    "            Income,\n",
    "            PERCENT_RANK() OVER (partition by Department ORDER BY Income) AS percentile_rank\n",
    "        FROM hremployee\n",
    "    ) AS Percentiles\n",
    ") \n",
    "GROUP BY category\n",
    "ORDER by category\n",
    "'''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926ec61",
   "metadata": {},
   "source": [
    "### Hive Integration with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark integration with Hive Warehouse\n",
    "# congif for Hive-integration property-name \"spark.sql.warehouse.dir\"\n",
    "# value = \"/user/hive/warehouse\"\n",
    "spark = (SparkSession.builder.appName(\"pyspark-Hive-Integration\")\n",
    "        .config(\"spark.sql.warehouse.dir\",\"/usr/hive/warehouse\")\n",
    "        .enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0693fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS airlines\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6080942",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SHOW databases\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" USE AIRLINES\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS FLIGHTS(DayofMonth INT,\n",
    "DayOfWeek INT,Carrier VARCHAR(10),OriginAirportID INT,DestAirportID INT,DepDelay INT,ArrDelay INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES('skip.header.line.count'='1')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20980a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SHOW TABLES''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225da888",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"LOAD DATA LOCAL INPATH '/home/hadoop/Downloads/raw_flight_data1.csv'\n",
    "OVERWRITE INTO TABLE flights\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24000961",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS airports(airport_id INT,city VARCHAR(50),state VARCHAR(50),name VARCHAR(50))\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES('skip.header.line.count'='1')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"LOAD DATA LOCAL INPATH '/home/hadoop/Downloads/airports1.csv'\n",
    "OVERWRITE INTO TABLE airports\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select * from airports\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select * from flights\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" select count(*) from flights\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dda29a",
   "metadata": {},
   "source": [
    "### 1.Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1026355",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = spark.table('airlines.flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67e6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df = spark.table('airlines.airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f98b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2146a",
   "metadata": {},
   "source": [
    "### 2.Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebfff12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_df.join(airport_df,on = flights_df.OriginAirportID==airport_df.airport_id,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f072d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338138c9",
   "metadata": {},
   "source": [
    "### 3.Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c59b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join = flights_join.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db6bfae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/hadoop/Downloads/flights already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o89.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/hadoop/Downloads/flights already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-030b958f9a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights_join\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file:///home/hadoop/Downloads/flights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/hadoop/Downloads/flights already exists.;'"
     ]
    }
   ],
   "source": [
    "flights_join.write.parquet(\"file:///home/hadoop/Downloads/flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47049d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_parquet_df = spark.read.parquet(\"file:///home/hadoop/Downloads/flights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19158176",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8629f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_parquet_df.write.parquet(\"/flights1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7ae6bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://localhost:9000/airlines already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o160.parquet.\n: org.apache.spark.sql.AnalysisException: path hdfs://localhost:9000/airlines already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-67ad497ad176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflights_join\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Carrier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/airlines'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://localhost:9000/airlines already exists.;'"
     ]
    }
   ],
   "source": [
    "flights_join.write.partitionBy('Carrier').parquet('/airlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9acb1993",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+-----------------+-----+--------------------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|airport_id|             city|state|                name|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+-----------------+-----+--------------------+\n",
      "|         4|        7|     UA|          13930|        12953|     115|      91|     13930|          Chicago|   IL|Chicago O'Hare In...|\n",
      "|        28|        7|     AS|          14679|        13830|     -11|     -27|     14679|        San Diego|   CA|San Diego Interna...|\n",
      "|         3|        4|     WN|          11259|        12191|      28|      29|     11259|           Dallas|   TX|   Dallas Love Field|\n",
      "|        29|        6|     WN|          11292|        13204|       6|     -11|     11292|           Denver|   CO|Denver International|\n",
      "|         1|        3|     B6|          13204|        12478|      -3|      -1|     13204|          Orlando|   FL|Orlando Internati...|\n",
      "|         3|        6|     DL|          14831|        13487|      -1|       6|     14831|         San Jose|   CA|Norman Y. Mineta ...|\n",
      "|        22|        3|     UA|          10423|        12266|      -3|     -17|     10423|           Austin|   TX|Austin - Bergstro...|\n",
      "|         2|        1|     DL|          14831|        10397|      -5|      -4|     14831|         San Jose|   CA|Norman Y. Mineta ...|\n",
      "|        16|        2|     AA|          11298|        11278|       2|      26|     11298|Dallas/Fort Worth|   TX|Dallas/Fort Worth...|\n",
      "|         1|        2|     WN|          11292|        13198|      -3|     -13|     11292|           Denver|   CO|Denver International|\n",
      "|        30|        4|     F9|          11292|        12892|      74|      82|     11292|           Denver|   CO|Denver International|\n",
      "|        22|        1|     MQ|          11066|        12953|     104|    null|     11066|         Columbus|   OH|Port Columbus Int...|\n",
      "|        20|        1|     WN|          13232|        11292|       2|     -15|     13232|          Chicago|   IL|Chicago Midway In...|\n",
      "|        19|        6|     WN|          14057|        13232|       3|      -6|     14057|         Portland|   OR|Portland Internat...|\n",
      "|        19|        5|     EV|          13244|        11057|       1|      14|     13244|          Memphis|   TN|Memphis Internati...|\n",
      "|         4|        7|     B6|          11618|        11697|      -1|     -19|     11618|           Newark|   NJ|Newark Liberty In...|\n",
      "|         1|        6|     DL|          15304|        11433|       0|      -3|     15304|            Tampa|   FL| Tampa International|\n",
      "|        25|        4|     WN|          12892|        12889|      -2|     -13|     12892|      Los Angeles|   CA|Los Angeles Inter...|\n",
      "|         5|        6|     OO|          10423|        12266|      -8|     -16|     10423|           Austin|   TX|Austin - Bergstro...|\n",
      "|        12|        3|     MQ|          11298|        12191|      14|      16|     11298|Dallas/Fort Worth|   TX|Dallas/Fort Worth...|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+----------+-----------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56fa09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.bucketBy(col='state',numBuckets = 50).format(\"csv\").saveAsTable(\"bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0d12f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_join.write.partitionBy('Carrier').bucketBy(col = 'state',numBuckets = 30)\\\n",
    ".format(\"parquet\").saveAsTable(\"part_bucket_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9900c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|carrier|count(1)|\n",
      "+-------+--------+\n",
      "|     UA|  287601|\n",
      "|     AA|  291771|\n",
      "|     EV|  158253|\n",
      "|     B6|  122297|\n",
      "|     DL|  385040|\n",
      "|     OO|  161102|\n",
      "|     F9|   35821|\n",
      "|     YV|   53022|\n",
      "|     US|  235031|\n",
      "|     MQ|  113634|\n",
      "|     HA|   18658|\n",
      "|     AS|   69056|\n",
      "|     FL|   93013|\n",
      "|     VX|   34869|\n",
      "|     WN|  580029|\n",
      "|     9E|   80221|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select carrier,count(*) from part_bucket_table group by carrier\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2fe473",
   "metadata": {},
   "source": [
    "### Load on MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9d310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_properties = {\n",
    "    'user':\"root\",\n",
    "    'password':\"hadoop@123\",\n",
    "    'driver':'com.mysql.cj.jdbc.Driver'\n",
    "}\n",
    "flights_join.write.jdbc(url = \"jdbc:mysql://localhost:3306/flights\", table = \"airlines\",\n",
    "                        mode = \"overwrite\",properties = connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "502fe6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31014587",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a3463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
